{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Pytorch Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the third day of the pool. In this part you will learn how to build neural networks using Pytorch.\n",
    "\n",
    "\n",
    "You've already learned how to create a neural networks from scratch, this involves digging deep down in math and and is quite complex at some levels.\n",
    "In this exercise you will learn to fly by all the complexity of AI by using the Pytorch library.\n",
    "\n",
    "Pytorch is a Python library used to create simple or complex AI as fast and efficient as possible.\n",
    "\n",
    "\n",
    "The first step will be to install and import the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /mnt/nfs/homes/lboukrou/.local/lib/python3.8/site-packages (1.10.2)\r\n",
      "Requirement already satisfied: typing-extensions in /mnt/nfs/homes/lboukrou/.local/lib/python3.8/site-packages (from torch) (4.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Pytorch and tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first part will be to learn about Tensor and learn to use them. A Tensor is a multi-dimensional matrix containing any numerical value. We can see a tensor like a multi-dimensional array, it can have any shape.\n",
    "\n",
    "\n",
    "**Exercise :**\\\n",
    "Create a tensor containing  only ones, the tensor must have a shape of ``[3, 3]``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use ones fonction of Pytorch librairie.\n",
    "torch.ones([3,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected result :**\n",
    "`tensor([[1., 1., 1.],\n",
    "        [1., 1., 1.],\n",
    "        [1., 1., 1.]])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you can create a tensor, let's try to do some simple math with it.\n",
    "\n",
    "**Exercise :**\\\n",
    "Create a tensor containing only ones, the tensor must have a shape of ``[3, 3]`` and find a way to multiply ``constant`` with the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 5., 5.],\n",
       "        [5., 5., 5.],\n",
       "        [5., 5., 5.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant = 5\n",
    "\n",
    "tensor = torch.ones([3,3])\n",
    "\n",
    "tensor * constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected result :**\n",
    "`tensor([[5., 5., 5.],\n",
    "        [5., 5., 5.],\n",
    "        [5., 5., 5.]])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to learn, is the ability to change the shape of the tensor.\n",
    "\n",
    "**Exercise :**\\\n",
    "Create a tensor containing  only ones, the tensor must have a shape of ``[3, 9]`` and reshape the tensor to a shape of ``[3, 3, 3]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.ones((3, 9), dtype=torch.float)\n",
    "# print(tensor)\n",
    "# reshape the tensor\n",
    "tensor.reshape([3,3,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected :**\\\n",
    "``tensor([[[1., 1., 1.],\n",
    "        [1., 1., 1.],\n",
    "        [1., 1., 1.]],\n",
    "        [[1., 1., 1.],\n",
    "        [1., 1., 1.],\n",
    "        [1., 1., 1.]],\n",
    "        [[1., 1., 1.],\n",
    "        [1., 1., 1.],\n",
    "        [1., 1., 1.]]])``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data for the AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you saw how easy it easy to create, and transform a tensor we can start creating an AI.\n",
    "\n",
    "From now on, our goal is going to create a simple AI whose purpose is to predict multiple of 2. For exemple, if the input is 5 the output should be 10 (or close enough).\n",
    "\n",
    "We must first start by create a function to get data to train and test our AI.\n",
    "\n",
    "**Exercise :**\\\n",
    "Create a function called `getData` which takes in two parameters `start` and `end`. The function must return a tendor (of shape `[n, 1]` with `n` equal to the number of interger) containing every interger between `start` (inclusive) and `end` (exclusive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [8.],\n",
      "        [9.]])\n",
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "def getData(start=0, end=200):\n",
    "    # use arange fonction of Pytorch librairie.\n",
    "    tensor = torch.arange(start, end, dtype=torch.float)\n",
    "    tensor = torch.reshape(tensor, [end, 1])\n",
    "    return tensor\n",
    "\n",
    "data = getData(0, 10)\n",
    "\n",
    "print(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected :**\\\n",
    "`tensor([[0.],\n",
    "        [1.],\n",
    "        [2.],\n",
    "        [3.],\n",
    "        [4.],\n",
    "        [5.],\n",
    "        [6.],\n",
    "        [7.],\n",
    "        [8.],\n",
    "        [9.]])`\\\n",
    "`torch.Size([10, 1])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have data for the input of the AI, but we need to compare the output data against the real expected result to compare both value and get the loss.\n",
    "\n",
    "**Exercise :**\\\n",
    "Create a function called `calcExpected` which takes one parameter `tensor`. The function must return a tensor where each value has been double."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.],\n",
       "        [ 2.],\n",
       "        [ 4.],\n",
       "        [ 6.],\n",
       "        [ 8.],\n",
       "        [10.],\n",
       "        [12.],\n",
       "        [14.],\n",
       "        [16.],\n",
       "        [18.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcExpected(tensor):\n",
    "    # multiply the tensor by 2\n",
    "    return tensor * 2\n",
    "\n",
    "data = getData(0, 10)\n",
    "calcExpected(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected :**\\\n",
    "`tensor([[ 0.],\n",
    "        [ 2.],\n",
    "        [ 4.],\n",
    "        [ 6.],\n",
    "        [ 8.],\n",
    "        [10.],\n",
    "        [12.],\n",
    "        [14.],\n",
    "        [16.],\n",
    "        [18.]])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buld a Pytorch neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done !\n",
    "\n",
    "Now that we have all our data, to train and test our AI, let's actually create it.\n",
    "I'm going to show you the simpliest model you can create as an exemple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExempleNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(1, 2) # First layer, it takes one number and output two number\n",
    "        self.linear2 = torch.nn.Linear(2, 1) # Second layer, it takes two number and output one number\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)             # Applies the first layer to the data\n",
    "        x = torch.nn.functional.relu(x) # Activation function\n",
    "        x = self.linear2(x)             # Applies the second layer to the data\n",
    "        x = torch.nn.functional.relu(x) # Activation function\n",
    "        return x\n",
    "\n",
    "network = ExempleNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see it's quite staight forward, we start by creating each layer of our AI (in this exemple, two layers are created), and we create a forward function to apply each layer with activation function after each layer.\n",
    "\n",
    "ReLU is an activation which looks like:\n",
    "<div>\n",
    "    <!-- <center> -->\n",
    "    <img src=\"../.img/ReLU.png\" width=\"300\" style=\"padding-left: 20px;\"/>\n",
    "    <!-- </center> -->\n",
    "</div>\n",
    "\n",
    "Let's test to see if our model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1629], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = getData(0, 1)\n",
    "network.forward(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected :** No particular value is expected because it is a random output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have an exemple of how Pytorch AI models are constructed, it's your turn to create your own AI.\n",
    "\n",
    "**Exercise :** Create a AI model, which must take one number as input and output one number. You can create as much layers as you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # create your layers\n",
    "        self.linear1 = torch.nn.Linear(1, 2) # First layer, it takes one number and output two number\n",
    "        self.linear2 = torch.nn.Linear(2, 1) # Second layer, it takes two number and output one number\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # apply each layer with a activation function\n",
    "        x = self.linear1(x)             # Applies the first layer to the data\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "network = MyNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4308], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = getData(0, 1)\n",
    "network.forward(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected :** No particular value is expected because it is a random output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before combining everyhting we've learn, we need to declare a loss function, and a optimizer. There is a vast amount amount of loss function and optimizer..\n",
    "\n",
    "An optimizer is gonna let us apply the gradiant on the model. The optimizer is comprised of a algorithm to apply the gradients. The algorithm that we are going to use is Adam. (for more information, check the following link: <a>https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam</a>\n",
    "\n",
    "For the loss function, the task we are trying to solve is a regression problem, thus the mean squared error function is a good choice, its formula is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*} & Loss(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "\n",
    "mse = torch.nn.MSELoss() # Loss function\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=LR) # Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the elements to train and test our AI.\n",
    "\n",
    "To summarize every element we've created:\n",
    "\n",
    "    - Create the model of our AI, and create a forward function to get the AI's prediction.\n",
    "    - Chose and initialize a loss function adapted to the goal of our task.\n",
    "    - Chose and initialize a optimizer to apply the gradients to improve the AI.\n",
    "\n",
    "\n",
    "Let's start by training our AI, we will first load our data.\n",
    "\n",
    "(The next code cell is already completed, but read the comments carefully to make sure everything is clear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs/homes/lboukrou/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:154: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1856, grad_fn=<MseLossBackward0>)\n",
      "tensor(508.0105, grad_fn=<MseLossBackward0>)\n",
      "tensor(1667.3236, grad_fn=<MseLossBackward0>)\n",
      "tensor(2957.3418, grad_fn=<MseLossBackward0>)\n",
      "tensor(3901.2468, grad_fn=<MseLossBackward0>)\n",
      "tensor(4115.8560, grad_fn=<MseLossBackward0>)\n",
      "tensor(3239.9404, grad_fn=<MseLossBackward0>)\n",
      "tensor(1351.2170, grad_fn=<MseLossBackward0>)\n",
      "tensor(33.0279, grad_fn=<MseLossBackward0>)\n",
      "tensor(187.1589, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.8963, grad_fn=<MseLossBackward0>)\n",
      "tensor(32.8925, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2900, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3375, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9715, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6071, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2041, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0686, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0118, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.7123e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.6069e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6915e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0863e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.5714e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3793e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3064e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1330e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3448e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0766e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0766e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.5367e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.3819e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5076e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3644e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3644e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0175e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3842e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3411e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0175e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7253e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3411e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9605e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3411e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3411e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9605e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8254e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9605e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9605e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3842e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3411e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(0., grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4901e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(0., grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4901e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4901e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3842e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3842e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(0., grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9605e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9605e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3644e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7253e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8280e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.5831e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "tensor(21.7671, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2420, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0433, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1539, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1023, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2039, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0567, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0197, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0084, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.8059, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.5430, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1838, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2719, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0283, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3832, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0449, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0119, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8414e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0104, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7746, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5682, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.0724, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6619, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3189, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.7406, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.1511, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.1913, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5757, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7247, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0453, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3701, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4293, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0189, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0181, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0113, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.4379e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.2474e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.9408e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9143e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7552e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.3621e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1517e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3862e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2317e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.8804e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2122e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6662e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8147e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.3528e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5183e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9707e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9707e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8030e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4901e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0766e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2070e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.3016e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3644e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.2957e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3644e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3842e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3644e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3842e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0175e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0175e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.3132e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8254e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7253e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9605e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7253e-09, grad_fn=<MseLossBackward0>)\n",
      "tensor(0., grad_fn=<MseLossBackward0>)\n",
      "tensor(0., grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9605e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9605e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4901e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3411e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4901e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9605e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3411e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9605e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4901e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4901e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3411e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7253e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.3016e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4901e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3842e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9605e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3842e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3644e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7253e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9605e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3644e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9605e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.3016e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7253e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.3016e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2070e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.3016e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3644e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1458e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2122e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0999, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1314, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.9908, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.1897, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3548, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2268, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1365, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1190, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0175, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0068, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2532e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3644e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0462, grad_fn=<MseLossBackward0>)\n",
      "tensor(48.3493, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 2\n",
    "data = getData(0, 1001) # get our training data\n",
    "\n",
    "for j in range(EPOCH):\n",
    "    for i in range(len(data)):\n",
    "        output = network.forward(data[i]) # Forward pass\n",
    "        loss = mse(output, calcExpected(data[i]))\n",
    "        loss.backward() # Calculating each gradient\n",
    "        optimizer.step() # Apply each gradient\n",
    "        optimizer.zero_grad() # Reset gradient to zero\n",
    "        if (i % 10 == 0):\n",
    "            print(loss) # Print the loss every 10 cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "After training our AI, let's test its accuracy on new data which the AI hasn't been trained on.\n",
    "\n",
    "(The next code cell is already completed, but read the comments carefully to make sure everything is clear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getData(2000, 2101) # get our testing data\n",
    "for i in range(len(data)):\n",
    "    output = network.forward(data[i]) # Forward pass\n",
    "    print('Prediction %.2f, Expected %.2f' % (output.item(), calcExpected(data[i]).item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected :** The prediction of the AI must be close to the expected value, if the difference between the prediction and expected is higher than 5, try to improve your AI model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratz\n",
    "\n",
    "Congratulations for having reached the end of this workshop !\\\n",
    "You have been able to create your own AI using Pytorch.\\\n",
    "During this day, you'll have the chance to create a more complex AI with the fundamental you've just learned.\n",
    "\n",
    "See you for the next topic!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
